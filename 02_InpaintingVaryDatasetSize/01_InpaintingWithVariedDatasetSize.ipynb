{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting with Variable Dataset Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Image网 dataset consists of:\n",
    "\n",
    "1. A `/val` folder with 10 classes.\n",
    "2. A `/train` folder with 20 classes. \n",
    "  - There are ~125 images in each class that exists in `/val`. There are \n",
    "  - There are ~1,300 images in each class that does not exist in `/val`\n",
    "3. An `/unsup` folder with 7,750 unlabelled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question we would like to answer with this notebook is:\n",
    "\n",
    "> What is the effect of dataset size during pretext training on downstream task performance?\n",
    "\n",
    "To answer this question we will consider four different datasets, each built from ImageWang.\n",
    "\n",
    "They are:\n",
    "\n",
    "1. All data in `/train`, `/unsup` and `/val`\n",
    "2. All data in `/train`, `/unsup`\n",
    "3. All data in `/train`\n",
    "4. Only Data in `/train` that has a corresponding class in `/val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from fastai2.layers import MishJit, MaxPool, LabelSmoothingCrossEntropy\n",
    "from fastai2.basics import DataBlock, RandomSplitter, GrandparentSplitter\n",
    "from fastai2.learner import Learner\n",
    "from fastai2.metrics import accuracy, top_k_accuracy\n",
    "from fastai2.optimizer import ranger, Adam, SGD, RMSProp\n",
    "\n",
    "from fastai2.vision.all import ImageBlock, PILMask, get_image_files, PILImage, imagenet_stats\n",
    "from fastai2.vision.core import get_annotations, Image, TensorBBox, TensorPoint, TensorImage\n",
    "from fastai2.vision.augment import aug_transforms, RandomResizedCrop, RandTransform, FlipItem\n",
    "from fastai2.vision.learner import unet_learner, unet_config\n",
    "from fastai2.vision.models.xresnet import xresnet50, xresnet34\n",
    "\n",
    "from fastai2.data.transforms import get_files\n",
    "from fastai2.data.external import download_url, URLs, untar_data\n",
    "\n",
    "from fastcore.foundation import L\n",
    "from fastcore.utils import num_cpus\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train this network with the best hyper-parameters/optimizer/settings we know. These settings come from [training Imagenette](https://github.com/fastai/imagenette/tree/58a63175a2c6457650289d32741940d6a7d58fbf). \n",
    "\n",
    "One thing to keep in mind is that the above is a classification task, so it's not 100% guaranteed that these settings will map perfectly to our task. That said, they're probably a very good starting point.\n",
    "\n",
    "As of January 2020 the [best parameters](https://github.com/fastai/imagenette/blob/58a63175a2c6457650289d32741940d6a7d58fbf/2020-01-train.md) are:\n",
    "\n",
    "```\n",
    "--lr 8e-3 \n",
    "--sqrmom 0.99 \n",
    "--mom 0.95 \n",
    "--eps 1e-6 \n",
    "--bs 64 \n",
    "--opt ranger \n",
    "--sa 1\n",
    "--fp16 1 \n",
    "--arch xse_resnext50 \n",
    "--pool MaxPool\n",
    "```\n",
    "\n",
    "One change we're making is that we're going to use **`xresnet34`** here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters\n",
    "gpu=None\n",
    "lr=1e-2\n",
    "size=128\n",
    "sqrmom=0.99\n",
    "mom=0.9\n",
    "eps=1e-6\n",
    "epochs=15\n",
    "bs=64\n",
    "mixup=0.\n",
    "opt='ranger',\n",
    "arch='xresnet50'\n",
    "sh=0.\n",
    "sa=0\n",
    "sym=0\n",
    "beta=0.\n",
    "act_fn='MishJit'\n",
    "fp16=0\n",
    "pool='AvgPool',\n",
    "dump=0\n",
    "runs=1\n",
    "meta=''\n",
    "\n",
    "# Chosen parameters\n",
    "lr=8e-3\n",
    "sqrmom=0.99\n",
    "mom=0.95\n",
    "eps=1e-6\n",
    "bs=64 \n",
    "opt='ranger'\n",
    "sa=1                 #NOTE: NOT USED HERE. Do we need this?\n",
    "fp16=0               #NOTE: My GPU cannot run fp16 :'(\n",
    "arch='xresnet50' \n",
    "pool='MaxPool'\n",
    "\n",
    "gpu=0\n",
    "\n",
    "# NOTE: Normally loaded from their corresponding string\n",
    "m = xresnet34\n",
    "act_fn = MishJit\n",
    "pool = MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create this dummy class in order to create a transform that ONLY operates on images of this type\n",
    "# We will use it to create all input images\n",
    "class PILImageInput(PILImage): pass\n",
    "\n",
    "class RandomCutout(RandTransform):\n",
    "    \"Picks a random scaled crop of an image and resize it to `size`\"\n",
    "    split_idx = None\n",
    "    def __init__(self, min_n_holes=5, max_n_holes=10, min_length=5, max_length=50, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_n_holes=min_n_holes\n",
    "        self.max_n_holes=max_n_holes\n",
    "        self.min_length=min_length\n",
    "        self.max_length=max_length\n",
    "        \n",
    "\n",
    "    def encodes(self, x:PILImageInput):\n",
    "        \"\"\"\n",
    "        Note that we're accepting our dummy PILImageInput class\n",
    "        fastai2 will only pass images of this type to our encoder. \n",
    "        This means that our transform will only be applied to input images and won't\n",
    "        be run against output images.\n",
    "        \"\"\"\n",
    "        \n",
    "        n_holes = np.random.randint(self.min_n_holes, self.max_n_holes)\n",
    "        pixels = np.array(x) # Convert to mutable numpy array. FeelsBadMan\n",
    "        h,w = pixels.shape[:2]\n",
    "\n",
    "        for n in range(n_holes):\n",
    "            h_length = np.random.randint(self.min_length, self.max_length)\n",
    "            w_length = np.random.randint(self.min_length, self.max_length)\n",
    "            h_y = np.random.randint(0, h)\n",
    "            h_x = np.random.randint(0, w)\n",
    "            y1 = int(np.clip(h_y - h_length / 2, 0, h))\n",
    "            y2 = int(np.clip(h_y + h_length / 2, 0, h))\n",
    "            x1 = int(np.clip(h_x - w_length / 2, 0, w))\n",
    "            x2 = int(np.clip(h_x + w_length / 2, 0, w))\n",
    "           \n",
    "            pixels[y1:y2, x1:x2, :] = 0\n",
    "            \n",
    "        return Image.fromarray(pixels, mode='RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = untar_data(URLs.IMAGEWANG_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers for creating the data bunch\n",
    "workers = min(8, num_cpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms are the same for each experiment\n",
    "item_tfms=[RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5), RandomCutout]\n",
    "batch_tfms=RandomErasing(p=0.9, max_count=3, sh=sh) if sh else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 160\n",
    "#CHANGE: I can only fit ~32 images in a batch\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Items From Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before we do anything, let's create some helper methods that will give us only the training sets that we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Files: 26348\n",
      "Train Files: 14669\n",
      "Unsup Files: 7750\n",
      "Valid Files: 3929\n",
      "\n",
      "Train+Unsup Files: 22419\n",
      "Train+Unsup Files: 1275\n"
     ]
    }
   ],
   "source": [
    "def get_all_items(path):\n",
    "    return get_files(path, extensions='.JPEG', recurse=True)\n",
    "\n",
    "def get_train_items(path):\n",
    "    return get_files(path/'train', extensions='.JPEG', recurse=True)\n",
    "\n",
    "def get_unsup_items(path):\n",
    "    return get_files(path/'unsup', extensions='.JPEG', recurse=True)\n",
    "\n",
    "def get_valid_items(path):\n",
    "    return get_files(path/'val', extensions='.JPEG', recurse=True)\n",
    "\n",
    "def get_train_and_unsup(path):\n",
    "    return get_train_items(path) + get_unsup_items(path)\n",
    "\n",
    "def get_train_items_that_are_present_in_val(path):\n",
    "    \"\"\"\n",
    "    We first get a list of all classes in /val\n",
    "    Then we use that list to get all the examples of each class from /train\n",
    "    \"\"\"\n",
    "    val = source/'val'\n",
    "    validation_classes = [path.name for path in val.iterdir()]\n",
    "    \n",
    "    train_files = L()\n",
    "    for class_name in validation_classes:\n",
    "        items = get_files(path/'train'/class_name, extensions='.JPEG', recurse=True)\n",
    "        train_files = train_files + items\n",
    "        \n",
    "    return train_files\n",
    "\n",
    "all_items = get_all_items(untar_data(URLs.IMAGEWANG_160))\n",
    "train_items = get_train_items(untar_data(URLs.IMAGEWANG_160))\n",
    "unsup_items = get_unsup_items(untar_data(URLs.IMAGEWANG_160))\n",
    "valid_items = get_valid_items(untar_data(URLs.IMAGEWANG_160))\n",
    "\n",
    "print(\"All Files: {}\".format(len(all_items)))\n",
    "print(\"Train Files: {}\".format(len(train_items)))\n",
    "print(\"Unsup Files: {}\".format(len(unsup_items)))\n",
    "print(\"Valid Files: {}\".format(len(valid_items)))\n",
    "print()\n",
    "\n",
    "train_and_unsup_items = get_train_and_unsup(untar_data(URLs.IMAGEWANG_160))\n",
    "print(\"Train+Unsup Files: {}\".format(len(train_and_unsup_items)))\n",
    "train_in_valid_items = get_train_items_that_are_present_in_val(untar_data(URLs.IMAGEWANG_160))\n",
    "print(\"Train+Unsup Files: {}\".format(len(train_in_valid_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all data in `/train`, `/unsup` and `/val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 26348\n",
      "Validation Size: 0\n"
     ]
    }
   ],
   "source": [
    "dblock = DataBlock(blocks=(ImageBlock(cls=PILImageInput), ImageBlock),\n",
    "                   splitter=RandomSplitter(valid_pct=0),\n",
    "                   get_items=get_all_items, \n",
    "                   get_y=lambda o: o)\n",
    "\n",
    "dbunch =  dblock.databunch(source, path=source, bs=bs, num_workers=workers, \n",
    "                        item_tfms=item_tfms, batch_tfms=batch_tfms)\n",
    "\n",
    "#CHANGE: We're predicting pixel values, so we're just going to predict an output for each RGB channel\n",
    "dbunch.vocab = ['R', 'G', 'B']\n",
    "\n",
    "print(\"Training Size:\", len(dbunch.train_ds))\n",
    "print(\"Validation Size:\", len(dbunch.valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='15', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/15 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='286' class='' max='823', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      34.75% [286/823 00:57<01:47 0.0901]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = unet_learner(dbunch, m, opt_func=opt_func, metrics=[], loss_func=MSELoss())\n",
    "if dump: print(learn.model); exit()\n",
    "if fp16: learn = learn.to_fp16()\n",
    "cbs = MixUp(mixup) if mixup else []\n",
    "learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)\n",
    "\n",
    "# I'm not using fastai2's .export() because I only want to save \n",
    "# the model's parameters. \n",
    "torch.save(learn.model[0].state_dict(), 'all_train_unsup_val_pretext.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all data in `/train` and `/unsup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dblock = DataBlock(blocks=(ImageBlock(cls=PILImageInput), ImageBlock),\n",
    "                   splitter=RandomSplitter(valid_pct=0),\n",
    "                   get_items=get_train_and_unsup, \n",
    "                   get_y=lambda o: o)\n",
    "dbunch =  dblock.databunch(source, path=source, bs=bs, num_workers=workers, \n",
    "                        item_tfms=item_tfms, batch_tfms=batch_tfms)\n",
    "\n",
    "#CHANGE: We're predicting pixel values, so we're just going to predict an output for each RGB channel\n",
    "dbunch.vocab = ['R', 'G', 'B']\n",
    "\n",
    "print(\"Training Size:\", len(dbunch.train_ds))\n",
    "print(\"Validation Size:\", len(dbunch.valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(dbunch, m, opt_func=opt_func, metrics=[], loss_func=MSELoss())\n",
    "if dump: print(learn.model); exit()\n",
    "if fp16: learn = learn.to_fp16()\n",
    "cbs = MixUp(mixup) if mixup else []\n",
    "learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)\n",
    "\n",
    "# I'm not using fastai2's .export() because I only want to save \n",
    "# the model's parameters. \n",
    "torch.save(learn.model[0].state_dict(), 'all_train_unsup_pretext.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all data in `/train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks=(ImageBlock(cls=PILImageInput), ImageBlock),\n",
    "                   splitter=RandomSplitter(valid_pct=0),\n",
    "                   get_items=get_train_items, \n",
    "                   get_y=lambda o: o)\n",
    "\n",
    "dbunch =  dblock.databunch(source, path=source, bs=bs, num_workers=workers, \n",
    "                        item_tfms=item_tfms, batch_tfms=batch_tfms)\n",
    "\n",
    "#CHANGE: We're predicting pixel values, so we're just going to predict an output for each RGB channel\n",
    "dbunch.vocab = ['R', 'G', 'B']\n",
    "\n",
    "print(\"Training Size:\", len(dbunch.train_ds))\n",
    "print(\"Validation Size:\", len(dbunch.valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(dbunch, m, opt_func=opt_func, metrics=[], loss_func=MSELoss())\n",
    "if dump: print(learn.model); exit()\n",
    "if fp16: learn = learn.to_fp16()\n",
    "cbs = MixUp(mixup) if mixup else []\n",
    "learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)\n",
    "\n",
    "# I'm not using fastai2's .export() because I only want to save \n",
    "# the model's parameters. \n",
    "torch.save(learn.model[0].state_dict(), 'all_train_pretext.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with partial data from `/train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks=(ImageBlock(cls=PILImageInput), ImageBlock),\n",
    "                   splitter=RandomSplitter(valid_pct=0),\n",
    "                   get_items=get_train_items_that_are_present_in_val, \n",
    "                   get_y=lambda o: o)\n",
    "\n",
    "dbunch =  dblock.databunch(source, path=source, bs=bs, num_workers=workers, \n",
    "                        item_tfms=item_tfms, batch_tfms=batch_tfms)\n",
    "\n",
    "#CHANGE: We're predicting pixel values, so we're just going to predict an output for each RGB channel\n",
    "dbunch.vocab = ['R', 'G', 'B']\n",
    "\n",
    "print(\"Training Size:\", len(dbunch.train_ds))\n",
    "print(\"Validation Size:\", len(dbunch.valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(dbunch, m, opt_func=opt_func, metrics=[], loss_func=MSELoss())\n",
    "if dump: print(learn.model); exit()\n",
    "if fp16: learn = learn.to_fp16()\n",
    "cbs = MixUp(mixup) if mixup else []\n",
    "learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)\n",
    "\n",
    "# I'm not using fastai2's .export() because I only want to save \n",
    "# the model's parameters. \n",
    "torch.save(learn.model[0].state_dict(), 'partial_train_pretext.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Task: Image网"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained models on our pretext tasks, let's compare the performance of each model against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbunch(size, bs, sh=0., workers=None):\n",
    "    if size<=224: \n",
    "        path = URLs.IMAGEWANG_160\n",
    "    else: \n",
    "        path = URLs.IMAGEWANG\n",
    "    source = untar_data(path)\n",
    "    if workers is None: workers = min(8, num_cpus())\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                       splitter=GrandparentSplitter(valid_name='val'),\n",
    "                       get_items=get_image_files, get_y=parent_label)\n",
    "    item_tfms=[RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n",
    "    batch_tfms=RandomErasing(p=0.9, max_count=3, sh=sh) if sh else None\n",
    "    return dblock.databunch(source, path=source, bs=bs, num_workers=workers,\n",
    "                            item_tfms=item_tfms, batch_tfms=batch_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data in `/train`, `/unsup` and `/val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(runs):\n",
    "        print(f'Run: {run}')\n",
    "        #CHANGE: No self-attention\n",
    "        sa = 0\n",
    "        learn = Learner(dbunch, m(c_out=20, act_cls=torch.nn.ReLU, sa=sa, sym=sym, pool=pool), opt_func=opt_func, \\\n",
    "                metrics=[accuracy,top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n",
    "        if dump: print(learn.model); exit()\n",
    "        if fp16: learn = learn.to_fp16()\n",
    "        cbs = MixUp(mixup) if mixup else []\n",
    "            \n",
    "        # Load weights generated from training on our pretext task\n",
    "        state_dict = torch.load('all_train_unsup_val_pretext.pth')\n",
    "        # HACK: If we don't have all of the parameters for our learner, we get an error\n",
    "        linear_layer = learn.model[-1]\n",
    "        state_dict['11.weight'] = linear_layer.weight\n",
    "        state_dict['11.bias'] = linear_layer.bias\n",
    "        \n",
    "        learn.model.load_state_dict(state_dict)\n",
    "        \n",
    "        learn.freeze()\n",
    "        learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data in `/train` and `/unsup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(runs):\n",
    "        print(f'Run: {run}')\n",
    "        #CHANGE: No self-attention\n",
    "        sa = 0\n",
    "        learn = Learner(dbunch, m(c_out=20, act_cls=torch.nn.ReLU, sa=sa, sym=sym, pool=pool), opt_func=opt_func, \\\n",
    "                metrics=[accuracy,top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n",
    "        if dump: print(learn.model); exit()\n",
    "        if fp16: learn = learn.to_fp16()\n",
    "        cbs = MixUp(mixup) if mixup else []\n",
    "            \n",
    "        # Load weights generated from training on our pretext task\n",
    "        state_dict = torch.load('all_train_unsup_pretext.pth')\n",
    "        # HACK: If we don't have all of the parameters for our learner, we get an error\n",
    "        linear_layer = learn.model[-1]\n",
    "        state_dict['11.weight'] = linear_layer.weight\n",
    "        state_dict['11.bias'] = linear_layer.bias\n",
    "        \n",
    "        learn.model.load_state_dict(state_dict)\n",
    "        \n",
    "        learn.freeze()\n",
    "        learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data in `/train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(runs):\n",
    "        print(f'Run: {run}')\n",
    "        #CHANGE: No self-attention\n",
    "        sa = 0\n",
    "        learn = Learner(dbunch, m(c_out=20, act_cls=torch.nn.ReLU, sa=sa, sym=sym, pool=pool), opt_func=opt_func, \\\n",
    "                metrics=[accuracy,top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n",
    "        if dump: print(learn.model); exit()\n",
    "        if fp16: learn = learn.to_fp16()\n",
    "        cbs = MixUp(mixup) if mixup else []\n",
    "            \n",
    "        # Load weights generated from training on our pretext task\n",
    "        state_dict = torch.load('all_train_pretext.pth')\n",
    "        # HACK: If we don't have all of the parameters for our learner, we get an error\n",
    "        linear_layer = learn.model[-1]\n",
    "        state_dict['11.weight'] = linear_layer.weight\n",
    "        state_dict['11.bias'] = linear_layer.bias\n",
    "        \n",
    "        learn.model.load_state_dict(state_dict)\n",
    "        \n",
    "        learn.freeze()\n",
    "        learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial data from `/train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(runs):\n",
    "        print(f'Run: {run}')\n",
    "        #CHANGE: No self-attention\n",
    "        sa = 0\n",
    "        learn = Learner(dbunch, m(c_out=20, act_cls=torch.nn.ReLU, sa=sa, sym=sym, pool=pool), opt_func=opt_func, \\\n",
    "                metrics=[accuracy,top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n",
    "        if dump: print(learn.model); exit()\n",
    "        if fp16: learn = learn.to_fp16()\n",
    "        cbs = MixUp(mixup) if mixup else []\n",
    "            \n",
    "        # Load weights generated from training on our pretext task\n",
    "        state_dict = torch.load('partial_train_pretext.pth')\n",
    "        # HACK: If we don't have all of the parameters for our learner, we get an error\n",
    "        linear_layer = learn.model[-1]\n",
    "        state_dict['11.weight'] = linear_layer.weight\n",
    "        state_dict['11.bias'] = linear_layer.bias\n",
    "        \n",
    "        learn.model.load_state_dict(state_dict)\n",
    "        \n",
    "        learn.freeze()\n",
    "        learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai2)",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
